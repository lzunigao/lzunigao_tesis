{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from my_net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = r'C:\\Users\\lzuni\\Documents\\Tesis\\database'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        csv_path: str | Path,\n",
    "        image_folder: str | Path,\n",
    "        \n",
    "        \n",
    "    ) -> None:\n",
    "        \n",
    "        if not isinstance(image_folder, Path):\n",
    "            image_folder = Path(image_folder)\n",
    "\n",
    "        self._transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: self.normalize_ct(x)),  \n",
    "            transforms.Resize((61, 61)),\n",
    "        ])\n",
    "        self.labels_from_file =  list(pd.read_csv(csv_path)[\"Label\"])\n",
    "        self.labels = torch.zeros((len(self.labels_from_file), max(self.labels_from_file)))\n",
    "        for i, l in enumerate(self.labels_from_file):\n",
    "            self.labels[i, l-1] = 1\n",
    "\n",
    "        \n",
    "        images = [Image.open(image_folder / f) for f in os.listdir(image_folder) if \".tiff\" in f]\n",
    "        self.data = [self._transforms(i) for i in images]\n",
    "        \n",
    "    def normalize_ct(self,image, min_val=-1000, max_val=100):\n",
    "        image = torch.clamp(image, min_val, max_val)\n",
    "        return (image - min_val) / (max_val - min_val)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor,torch.Tensor]:\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_str(labels):\n",
    "    str_labels = []\n",
    "\n",
    "    for l in labels:\n",
    "        if torch.equal(l, torch.tensor([1., 0., 0.])):\n",
    "            str_labels.append('NT')\n",
    "        elif torch.equal(l, torch.tensor([0., 1., 0.])):\n",
    "            str_labels.append('CLE')\n",
    "        elif torch.equal(l, torch.tensor([0., 0., 1.])):\n",
    "            str_labels.append('PSE')\n",
    "\n",
    "    return str_labels\n",
    "\n",
    "\n",
    "def imshow(images, labels, label_names):\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(3*len(images)+1, 4))\n",
    "    axes = np.atleast_1d(axes)\n",
    "    for idx, (img, label) in enumerate(zip(images, labels)):\n",
    "        npimg = np.array(img)\n",
    "        axes[idx].imshow(np.transpose(npimg, (1, 2, 0)), aspect='equal', cmap='gray')\n",
    "        axes[idx].set_title(label_names[idx])\n",
    "        axes[idx].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def dataset_reader(csv_path, folder_path, dataset_type):\n",
    "\n",
    "    dataset = PatchesDataset(csv_path, folder_path)\n",
    "    ratio = 0.8\n",
    "    indices = []\n",
    "    aux = []\n",
    "\n",
    "    for label in range(1,4):\n",
    "        \n",
    "        aux = [i for i,j in enumerate(dataset.labels_from_file) if j == label]\n",
    "        \n",
    "        if dataset_type == 'train':\n",
    "            indices += aux[:int(len(aux)*ratio)]\n",
    "            print(aux[:int(len(aux)*ratio)])\n",
    "        elif dataset_type == 'test':\n",
    "            indices += aux[int(len(aux)*ratio):]\n",
    "            print(aux[int(len(aux)*ratio):])\n",
    "\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset, indices), batch_size=len(indices), shuffle=True)\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    #imshow(images, labels, get_labels_str(labels))\n",
    "    print(f'{dataset_type.capitalize()} dataset count: ', len(indices))\n",
    "    return dataset, loader\n",
    "\n",
    "def normalize_ct(image, min_val=-1000, max_val=100):\n",
    "    image = torch.clamp(image, min_val, max_val)\n",
    "    return (image - min_val) / (max_val - min_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
      "[59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]\n",
      "[109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]\n",
      "Train dataset count:  134\n",
      "[47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n",
      "[99, 100, 101, 102, 103, 104, 105, 106, 107, 108]\n",
      "[156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]\n",
      "Test dataset count:  34\n"
     ]
    }
   ],
   "source": [
    "database_path = r'C:\\Users\\lzuni\\Documents\\Tesis\\database\\patches'\n",
    "patch_labels_path = r'C:\\Users\\lzuni\\Documents\\Tesis\\database\\patch_labels.csv'\n",
    "\n",
    "train_dataset, train_loader = dataset_reader(patch_labels_path, database_path, 'train')\n",
    "test_dataset, test_loader = dataset_reader(patch_labels_path, database_path, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 5] [Analized images: 136] [Loss: 0.900]\n",
      "[Epoch: 10] [Analized images: 136] [Loss: 0.588]\n",
      "[Epoch: 15] [Analized images: 136] [Loss: 0.151]\n",
      "[Epoch: 20] [Analized images: 136] [Loss: 0.022]\n",
      "[Epoch: 25] [Analized images: 136] [Loss: 0.004]\n",
      "[Epoch: 30] [Analized images: 136] [Loss: 0.001]\n",
      "[Epoch: 35] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 40] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 45] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 50] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 55] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 60] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 65] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 70] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 75] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 80] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 85] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 90] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 95] [Analized images: 136] [Loss: 0.000]\n",
      "[Epoch: 100] [Analized images: 136] [Loss: 0.000]\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "net = Net(61)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 34 == 33 and epoch%5==4:   # print every 40 images\n",
    "            print(f'[Epoch: {epoch + 1}] [Analized images: {(i+1)*4}] [Loss: {running_loss/34 :.3f}]')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: ground truth =  NT , predicted  PSE\n",
      "Mismatch: ground truth =  CLE , predicted  NT\n",
      "Mismatch: ground truth =  NT , predicted  CLE\n",
      "Mismatch: ground truth =  CLE , predicted  PSE\n",
      "Mismatch: ground truth =  CLE , predicted  NT\n",
      "Accuracy:  85.294 %\n",
      "NT predictions:  11\n",
      "CLE predictions:  10\n",
      "PSE predictions:  13\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=34, shuffle=True)\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "#imshow(images, labels, label_names)\n",
    "\n",
    "outputs = net(images)\n",
    "prob_outputs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "predicted = torch.argmax(prob_outputs, dim=1)\n",
    "label_names = get_labels_str(labels)\n",
    "\n",
    "accuracy_count = 0\n",
    "nt_count = 0\n",
    "cle_count = 0\n",
    "pse_count = 0\n",
    "\n",
    "for idx, p in enumerate(predicted):\n",
    "    \n",
    "    \n",
    "    if p == 0:\n",
    "        lab = 'NT'\n",
    "        nt_count += 1\n",
    "    if p == 1:\n",
    "        lab = 'CLE'\n",
    "        cle_count += 1\n",
    "    if p == 2:\n",
    "        lab = 'PSE'\n",
    "        pse_count += 1\n",
    "    \n",
    "    if label_names[idx] == lab:\n",
    "        accuracy_count += 1\n",
    "    if label_names[idx] != lab:\n",
    "        print('Mismatch: ground truth = ', label_names[idx],', predicted ', lab)\n",
    "\n",
    "print('Accuracy: ', f'{accuracy_count*100/len(predicted):.3f}' , '%')\n",
    "print('NT predictions: ', nt_count)\n",
    "print('CLE predictions: ', cle_count)\n",
    "print('PSE predictions: ', pse_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
